# Beware of changing these parameters
training_ratio: 0.7
validation_ratio: 0.2
testing_ratio: 0.1

# Change these parameters as you like

# Whether or not to print network information, outputs, loss etc.
verbose: False

learning_rate: 0.001
batch_size: 32
epochs: 10

# List of neurons in each layer on the format [input, hidden, ..., hidden, output] of
# length 2-7, with values in the interval [1-1000]
# This means that we can have 0-5 hidden layers
neurons_in_each_layer: [10, 10, 10, 10]

# Layer type for each layer
# For example a network with 2 hidden layers: [recurrent, recurrent, dense]
layer_types: [recurrent, recurrent, dense]

# Activation function for each layer (except the input layer)
# Options [sigmoid, tanh, relu, linear]
# For example a network with 2 hidden layers: [sigmoid, relu, relu]
activation_functions: [relu, relu, linear]

softmax: False # Whether or not we should include a SoftMax layer at the end

# [MSE, cross_entropy] where MSE is mean-squared error
# Use cross_entropy if softmax is enabled for classification tasks
loss_function: MSE

# Initial weight ranges for each (non-input) layer,
# options: [glorot_normal, glorot_uniform, [low, high]]
# where [low, high] result in low <= weight <= high ranges.
initial_weight_ranges: glorot_normal

# Initial bias ranges on the form [low, high] such that low <= bias <= high
initial_bias_ranges: [0, 0]
