# Whether or not to print network information, outputs, loss etc.
verbose: False

learning_rate: 0.01
batch_size: 64
epochs: 10

# List of length 2-12, with values in the interval [1-1000]
# This means that we can have 0-10 hidden layers
neurons_in_each_layer: [625, 256, 256, 256, 256, 256, 4]

# Activation function for each layer (except the input layer)
# Options [sigmoid, tanh, relu, linear]
# For example a network with 2 hidden layers: [sigmoid, relu, relu]
activation_functions: [relu, relu, relu, relu, relu, linear]

softmax: True # Whether or not we should include a SoftMax layer at the end

# [MSE, cross_entropy] where MSE is mean-squared error
loss_function: cross_entropy

# [L1, L2, null]
global_weight_regularization_option: NULL

# Typically a small fraction
global_weight_regularization_rate: 0.001

# Initial weight ranges for each (non-input) layer,
# options: [glorot_normal, glorot_uniform, [low, high]]
# where [low, high] result in low <= weight <= high ranges.
initial_weight_ranges: glorot_normal

# Initial bias ranges on the form [low, high] such that low <= bias <= high
initial_bias_ranges: [0, 0]
