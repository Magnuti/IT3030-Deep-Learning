# Whether or not to print network information, outputs, loss etc.
verbose: True

learning_rate: 0.01
batch_size: 3

# List of length 2-12, with values between [1-1000]
# This means that we can have 0-10 hidden layers
neurons_in_each_layer: [625, 1000, 500, 800, 400, 4]

# Activation function for each layer (except the input layer)
# Options [sigmoid, tanh, relu, linear, softmax]
# Note that softmax is only applicable in the output layer
# For example a network with 2 hidden layers: [sigmoid, sigmoid, relu]
activation_functions: [linear, tanh, relu, sigmoid, softmax]

# [MSE, cross-entropy] where MSE is mean-squared error
loss_function: cross-entropy

# TODO should batch_size be here?

# [L1, L2, null]
global_weight_regularization_option: NULL

# Typically a small fraction
global_weight_regularization_rate: 0.001

# TODO look up glorot for weight_ranges
# Initial weight ranges for each (non-input) layer
initial_weight_ranges: [100, 100]

# Whether SoftMax should be included as the last layer
softmax: True

dataset_filename: dataset.txt
